{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title: VisDrone-DET2021: The Vision Meets Drone Object Detection Challenge Results\n",
    "\n",
    "#### Group Member Names : JORGE LUJANO PEREZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM :\n",
    "The aim of this project is to implement and analyze a machine learning model for object detection using the VisDrone-DET2021 dataset. We will explore the challenges of object detection in drone-captured imagery and evaluate the performance of our chosen model on this specific dataset.\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "https://github.com/VisDrone/VisDrone-Dataset\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The paper \"VisDrone-DET2021: The Vision Meets Drone Object Detection Challenge Results\" summarizes the results of the 2021 challenge. The challenge focused on object detection in drone-captured images and videos. The paper highlights the difficulties of this task, such as small objects, crowded scenes, and varied lighting conditions. It also describes the top-performing methods and the metrics used to evaluate them.\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "The primary problem is to accurately detect and classify objects of interest in images and videos captured by drones. The unique challenges of drone imagery, such as high-altitude perspective, small object sizes, and complex backgrounds, make this a difficult task for traditional object detection models.\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "The use of drones in various applications, including surveillance, traffic monitoring, and search and rescue operations, has created a demand for robust and accurate object detection systems. However, the characteristics of drone imagery present significant challenges that require specialized solutions.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "My solution will involve selecting and implementing an existing object detection model and training it on the VisDrone-DET2021 dataset. We will then evaluate its performance using the standard metrics from the challenge, such as Average Precision (AP), to demonstrate its effectiveness in addressing the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "##### Reference\n",
    "VisDrone-DET2021 Paper\n",
    "\n",
    "##### Explanation\n",
    "This paper outlines the results of the object detection challenge. It provides a comprehensive overview of the challenge and a summary of the best-performing methods.\n",
    "\n",
    "##### Dataset/Input\n",
    "The VisDrone-DET2021 dataset, which includes images and video frames captured by drones.\n",
    "\n",
    "##### Weakness\n",
    "The paper primarily serves as a summary of results rather than a detailed explanation of one specific method. It does not provide the code for a single winning solution, requiring us to choose a model to implement separately.\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "The code implementation will be based on a chosen object detection framework. We will use the provided dataset from the VisDrone-DET2021 GitHub repository to train and test our model. Our implementation will follow the standard training and evaluation pipelines for the chosen framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "Our contribution will involve fine-tuning the chosen model to optimize its performance on the VisDrone-DET2021 dataset. We will experiment with different hyperparameters and data augmentation techniques to improve the model's ability to detect small objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]: Wen, L., et al. (2021). \"VisDrone-DET2021: The Vision Meets Drone Object Detection Challenge Results.\" Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting C:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train: 6471it [01:49, 59.29it/s]\n",
      "Converting C:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-val: 548it [00:09, 60.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visdrone2yolo(dir):\n",
    "    \"\"\"Convert VisDrone annotations to YOLO format.\"\"\"\n",
    "    def convert_box(size, box):\n",
    "        # Convert VisDrone box to YOLO xywh box\n",
    "        dw = 1.0 / size[0]\n",
    "        dh = 1.0 / size[1]\n",
    "        return (box[0] + box[2] / 2) * dw, (box[1] + box[3] / 2) * dh, box[2] * dw, box[3] * dh\n",
    "\n",
    "    (dir / \"labels\").mkdir(parents=True, exist_ok=True)  # make labels directory\n",
    "    pbar = tqdm((dir / \"annotations\").glob(\"*.txt\"), desc=f\"Converting {dir}\")\n",
    "    for f in pbar:\n",
    "        img_size = Image.open((dir / \"images\" / f.name).with_suffix(\".jpg\")).size\n",
    "        lines = []\n",
    "        with open(f, encoding=\"utf-8\") as file:  # read annotation.txt\n",
    "            for row in [x.split(\",\") for x in file.read().strip().splitlines()]:\n",
    "                if row[4] == \"0\":  # VisDrone 'ignored regions' class 0\n",
    "                    continue\n",
    "                cls = int(row[5]) - 1\n",
    "                box = convert_box(img_size, tuple(map(int, row[:4])))\n",
    "                lines.append(f\"{cls} {' '.join(f'{x:.6f}' for x in box)}\\n\")\n",
    "        with open(str(f).replace(f\"{os.sep}annotations{os.sep}\", f\"{os.sep}labels{os.sep}\"), \"w\", encoding=\"utf-8\") as fl:\n",
    "            fl.writelines(lines)  # write label.txt\n",
    "\n",
    "# Path to your VisDrone dataset root directory\n",
    "visdrone_path = Path(\"C:\\\\Users\\\\lujan\\\\Documents\\\\AIDI 1002\\\\Final project\\\\VisDrone\")\n",
    "\n",
    "# Run the conversion for the train and val splits\n",
    "visdrone2yolo(visdrone_path / \"VisDrone2019-DET-train\")\n",
    "visdrone2yolo(visdrone_path / \"VisDrone2019-DET-val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujan\\anaconda3\\envs\\yolo_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.179 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.178  Python-3.10.18 torch-2.8.0+cpu CPU (AMD Athlon Silver 3050U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=VisDrone.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=8, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\lujan\\runs\\detect\\train11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING imgsz=[8] must be multiple of max stride 32, updating to [32]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 498.5526.6 MB/s, size: 261.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train\\labels.cache... 6471 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6471/6471 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train\\images\\0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train\\images\\0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train\\images\\9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-train\\images\\9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.60.7 ms, read: 109.023.2 MB/s, size: 131.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujan\\anaconda3\\envs\\yolo_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\\VisDrone2019-DET-val\\labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "C:\\Users\\lujan\\anaconda3\\envs\\yolo_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\lujan\\runs\\detect\\train11\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 32 train, 32 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\lujan\\runs\\detect\\train11\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/25         0G      1.462      1.578     0.2863          8         32: 100%|██████████| 405/405 [05:36<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:18<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/25         0G      3.219      2.873     0.5974         12         32: 100%|██████████| 405/405 [05:15<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:20<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000238   0.000229   0.000146   3.36e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/25         0G      3.809      3.109     0.6964         17         32: 100%|██████████| 405/405 [05:07<00:00,  1.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:18<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000315   0.000513   0.000162   2.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/25         0G      4.222      3.272     0.7491          8         32: 100%|██████████| 405/405 [05:12<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/25         0G      4.305      3.244     0.7514         11         32: 100%|██████████| 405/405 [05:07<00:00,  1.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:20<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000492   0.000367   0.000247   5.85e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/25         0G      4.324      3.181     0.7662          6         32: 100%|██████████| 405/405 [05:06<00:00,  1.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/25         0G      4.298      3.094     0.7541         17         32: 100%|██████████| 405/405 [05:05<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:18<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000247   0.000421   0.000125   2.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/25         0G       4.38      3.079     0.7674         10         32: 100%|██████████| 405/405 [05:00<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000806   0.000719   0.000409   0.000113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/25         0G      4.349      3.063     0.7571          8         32: 100%|██████████| 405/405 [04:44<00:00,  1.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00107   0.000594   0.000549   0.000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/25         0G      4.421      3.056     0.7641         14         32: 100%|██████████| 405/405 [04:45<00:00,  1.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000647   0.000529   0.000326   6.73e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/25         0G      4.338      3.055     0.7463          7         32: 100%|██████████| 405/405 [04:47<00:00,  1.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000703    0.00032    0.00042   9.54e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/25         0G      4.221      2.924     0.7308         52         32: 100%|██████████| 405/405 [04:46<00:00,  1.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:18<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759   0.000916   0.000831   0.000573   0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/25         0G      4.395      3.049      0.772         46         32:  17%|█▋        | 69/405 [00:49<05:24,  1.04it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "      15/25         0G      4.264      2.956     0.7399         12         32: 100%|██████████| 405/405 [05:21<00:00,  1.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00195   0.000594    0.00147   0.000271\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujan\\anaconda3\\envs\\yolo_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      16/25         0G      4.019      2.645     0.7078          1         32: 100%|██████████| 405/405 [04:55<00:00,  1.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00231   0.000288    0.00124   0.000217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/25         0G       4.02      2.595     0.7255          5         32: 100%|██████████| 405/405 [04:51<00:00,  1.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00196   0.000392    0.00111   0.000281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/25         0G      3.962      2.604     0.7039          5         32: 100%|██████████| 405/405 [04:39<00:00,  1.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00126    0.00051   0.000685   0.000167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/25         0G      3.922      2.596     0.7098          1         32: 100%|██████████| 405/405 [04:56<00:00,  1.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00105   0.000532   0.000544   0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/25         0G      3.981      2.632     0.7083         18         32: 100%|██████████| 405/405 [04:56<00:00,  1.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:18<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00188   0.000651   0.000954   0.000231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/25         0G      4.015      2.594     0.7173         24         32: 100%|██████████| 405/405 [04:59<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00102   0.000452   0.000527   0.000133\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/25         0G      3.833      2.498      0.693         12         32: 100%|██████████| 405/405 [04:54<00:00,  1.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00121   0.000452   0.000638    0.00014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/25         0G      3.898      2.533     0.7113          2         32: 100%|██████████| 405/405 [04:38<00:00,  1.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00166   0.000554   0.000884   0.000186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/25         0G      3.942      2.534      0.716          6         32: 100%|██████████| 405/405 [05:37<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:19<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00189    0.00105   0.000985   0.000197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/25         0G      3.895      2.536     0.7033          5         32: 100%|██████████| 405/405 [05:20<00:00,  1.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:15<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.002     0.0011    0.00107   0.000208\n",
      "\n",
      "25 epochs completed in 2.222 hours.\n",
      "Optimizer stripped from C:\\Users\\lujan\\runs\\detect\\train11\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\lujan\\runs\\detect\\train11\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\lujan\\runs\\detect\\train11\\weights\\best.pt...\n",
      "Ultralytics 8.3.178  Python-3.10.18 torch-2.8.0+cpu CPU (AMD Athlon Silver 3050U with Radeon Graphics)\n",
      "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:17<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759    0.00177   0.000579    0.00138   0.000263\n",
      "            pedestrian        520       8844          0          0          0          0\n",
      "                people        482       5125          0          0          0          0\n",
      "               bicycle        364       1287          0          0          0          0\n",
      "                   car        515      14064     0.0112   0.000782    0.00888     0.0021\n",
      "                   van        421       1975    0.00445    0.00101    0.00304   0.000304\n",
      "                 truck        266        750    0.00205      0.004    0.00185    0.00022\n",
      "              tricycle        337       1045          0          0          0          0\n",
      "       awning-tricycle        220        532          0          0          0          0\n",
      "                   bus        131        251          0          0          0          0\n",
      "                 motor        485       4886          0          0          0          0\n",
      "Speed: 0.1ms preprocess, 8.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\lujan\\runs\\detect\\train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the smallest YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "results = model.train(data=\"VisDrone.yaml\", epochs=25, imgsz=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastRCNNPredictor(\n",
      "  (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
      "  (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Step 1: Load a pre-trained Faster R-CNN model\n",
    "# We use the FasterRCNN with a ResNet50_FPN_V2 backbone.\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "\n",
    "# Step 2: Get the number of input features for the classifier head.\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Step 3: Update the classification head to match your number of classes.\n",
    "num_classes = 10 + 1\n",
    "\n",
    "# Replace the pre-trained head with a new one that has the correct number of classes.\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Step 4: Verify the change by printing the new classification head\n",
    "print(model.roi_heads.box_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujan\\AppData\\Local\\Temp\\ipykernel_16164\\75614936.py:114: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\lujan\\anaconda3\\envs\\yolo_env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# The VisDrone dataset has 10 classes + 1 background class.\n",
    "# The original dataset class IDs are 1-10. We will map them to 0-9.\n",
    "# The background class will be handled implicitly by the model.\n",
    "# The class map is based on the VisDrone documentation.\n",
    "VISDRONE_CLASSES = {\n",
    "    1: 'pedestrian', 2: 'person', 3: 'bicycle', 4: 'car', 5: 'van',\n",
    "    6: 'truck', 7: 'tricycle', 8: 'awning-tricycle', 9: 'bus', 10: 'motor'\n",
    "}\n",
    "NUM_CLASSES = 10 + 1\n",
    "\n",
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = os.path.join(self.root_dir, 'VisDrone2019-DET-' + split, 'images')\n",
    "        self.annotations_dir = os.path.join(self.root_dir, 'VisDrone2019-DET-' + split, 'annotations')\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.jpg')])\n",
    "        self.annotation_files = sorted([f for f in os.listdir(self.annotations_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        if len(self.image_files) != len(self.annotation_files):\n",
    "            print(f\"Warning: Number of images ({len(self.image_files)}) does not match number of annotations ({len(self.annotation_files)}).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        annotation_path = os.path.join(self.annotations_dir, self.annotation_files[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = list(map(int, line.strip().split(',')))\n",
    "                x_min, y_min, bbox_width, bbox_height = parts[0], parts[1], parts[2], parts[3]\n",
    "                object_category = parts[5]\n",
    "                \n",
    "                if object_category > 0:\n",
    "                    x_max = x_min + bbox_width\n",
    "                    y_max = y_min + bbox_height\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(object_category - 1)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        \n",
    "        # We handle the transforms in the DataLoader\n",
    "        if self.transforms:\n",
    "            img_tensor = F.to_tensor(img)\n",
    "            # Apply transforms with a dummy target to avoid issues with older torchvision\n",
    "            img_tensor, dummy_target = self.transforms(img_tensor, {})\n",
    "            # The transforms can change the bounding boxes, we need to handle that.\n",
    "            # However, for a simple Resize and Flip, the V2 transforms handle it automatically.\n",
    "            \n",
    "            # The V2 transforms for detection need the image and target to be passed together.\n",
    "            # A cleaner way is to handle transforms directly in __getitem__ for safety.\n",
    "            # Let's adjust the transform application slightly for robustness.\n",
    "            img = T.ToImage()(img)\n",
    "            img = T.ToDtype(torch.float32, scale=True)(img)\n",
    "            if self.transforms:\n",
    "                img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Corrected function to get data transforms\n",
    "def get_transform(train, image_size=(512, 512)):\n",
    "    transforms_list = [\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.ToPureTensor(),\n",
    "        T.Resize(image_size)\n",
    "    ]\n",
    "    if train:\n",
    "        transforms_list.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms_list)\n",
    "\n",
    "# Rest of the code remains the same.\n",
    "def get_model(num_classes):\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, data_loader, data_loader_test, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch} finished.\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = get_model(NUM_CLASSES)\n",
    "\n",
    "    root_dir = r\"C:\\Users\\lujan\\Documents\\AIDI 1002\\Final project\\VisDrone\"\n",
    "    image_size = (256, 256)\n",
    "    \n",
    "    # Corrected function call with the image_size argument\n",
    "    dataset = VisDroneDataset(root_dir=root_dir, split=\"train\", transforms=get_transform(train=True, image_size=image_size))\n",
    "    dataset_test = VisDroneDataset(root_dir=root_dir, split=\"val\", transforms=get_transform(train=False, image_size=image_size))\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    train_and_evaluate(model, data_loader, data_loader_test, device, num_epochs=10)\n",
    "\n",
    "    torch.save(model.state_dict(), \"faster_rcnn_visdrone.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:yolo_env]",
   "language": "python",
   "name": "conda-env-yolo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
